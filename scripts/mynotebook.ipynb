{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we explore the possibilities of fuzzy search algorithms in finding similarities.\n",
    "\n",
    "**Classification using fuzzy matching**\n",
    "\n",
    "-Classify whether question pairs are duplicate or not\n",
    "\n",
    "-Let us start with importing the necessary modules for exploring the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404290\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from pyxdameraulevenshtein import damerau_levenshtein_distance, normalized_damerau_levenshtein_distance\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "import random\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, ngrams\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "random.seed(1337)\n",
    "\n",
    "df_train = pd.read_csv('../data/train.csv', encoding=\"ISO-8859-1\")\n",
    "df_test = pd.read_csv('../data/test.csv', encoding=\"ISO-8859-1\")\n",
    "\n",
    "num_train = df_train.shape[0]\n",
    "print (num_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>How does the Surface Pro himself 4 compare wit...</td>\n",
       "      <td>Why did Microsoft choose core m3 and not core ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Should I have a hair transplant at age 24? How...</td>\n",
       "      <td>How much cost does hair transplant require?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What but is the best way to send money from Ch...</td>\n",
       "      <td>What you send money to China?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Which food not emulsifiers?</td>\n",
       "      <td>What foods fibre?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>How \"aberystwyth\" start reading?</td>\n",
       "      <td>How their can I start reading?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id                                          question1  \\\n",
       "0        0  How does the Surface Pro himself 4 compare wit...   \n",
       "1        1  Should I have a hair transplant at age 24? How...   \n",
       "2        2  What but is the best way to send money from Ch...   \n",
       "3        3                        Which food not emulsifiers?   \n",
       "4        4                   How \"aberystwyth\" start reading?   \n",
       "\n",
       "                                           question2  \n",
       "0  Why did Microsoft choose core m3 and not core ...  \n",
       "1        How much cost does hair transplant require?  \n",
       "2                      What you send money to China?  \n",
       "3                                  What foods fibre?  \n",
       "4                     How their can I start reading?  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Jaccard similarity and Levenshtein distance to find the similarity between the questions. The number of words matched are also used as features for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams generated...\n",
      "Bigrams generated...\n",
      "Word match share over...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
    "\n",
    "def get_unigrams(que):\n",
    "    return [word for word in word_tokenize(que.lower()) if word not in eng_stopwords]\n",
    "\n",
    "def get_common_unigrams(row):\n",
    "    return len( set(row[\"unigrams_ques1\"]).intersection(set(row[\"unigrams_ques2\"])) )\n",
    "\n",
    "def get_common_unigram_ratio(row):\n",
    "    return float(row[\"unigrams_common_count\"]) / max(len( set(row[\"unigrams_ques1\"]).union(set(row[\"unigrams_ques2\"])) ),1)\n",
    "\n",
    "df_all[\"unigrams_ques1\"] = df_all['question1'].apply(lambda x: get_unigrams(str(x)))\n",
    "df_all[\"unigrams_ques2\"] = df_all['question2'].apply(lambda x: get_unigrams(str(x)))\n",
    "df_all[\"unigrams_common_count\"] = df_all.apply(lambda row: get_common_unigrams(row),axis=1)\n",
    "df_all[\"unigrams_common_ratio\"] = df_all.apply(lambda row: get_common_unigram_ratio(row), axis=1)\n",
    "print (\"Unigrams generated...\")\n",
    "\n",
    "def get_bigrams(que):\n",
    "    return [i for i in ngrams(que, 2)]\n",
    "\n",
    "def get_common_bigrams(row):\n",
    "    return len( set(row[\"bigrams_ques1\"]).intersection(set(row[\"bigrams_ques2\"])) )\n",
    "\n",
    "def get_common_bigram_ratio(row):\n",
    "    return float(row[\"bigrams_common_count\"]) / max(len( set(row[\"bigrams_ques1\"]).union(set(row[\"bigrams_ques2\"])) ),1)\n",
    "\n",
    "df_all[\"bigrams_ques1\"] = df_all[\"unigrams_ques1\"].apply(lambda x: get_bigrams(x))\n",
    "df_all[\"bigrams_ques2\"] = df_all[\"unigrams_ques2\"].apply(lambda x: get_bigrams(x)) \n",
    "df_all[\"bigrams_common_count\"] = df_all.apply(lambda row: get_common_bigrams(row),axis=1)\n",
    "df_all[\"bigrams_common_ratio\"] = df_all.apply(lambda row: get_common_bigram_ratio(row), axis=1)\n",
    "\n",
    "df_all[\"words_ques1\"] = df_all[\"unigrams_ques1\"].apply(len)\n",
    "df_all[\"words_ques2\"] = df_all[\"unigrams_ques2\"].apply(len)\n",
    "print (\"Bigrams generated...\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "def word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "print (\"Word match share over...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\n",
    "test_qs = pd.Series(df_test['question1'].tolist() + df_test['question2'].tolist()).astype(str)\n",
    "dist_train = train_qs.apply(lambda x: len(x.split(' ')))\n",
    "dist_test = test_qs.apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# If a word appears only once, we ignore it completely (likely a typo)\n",
    "# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "\n",
    "eps = 5000 \n",
    "words = (\" \".join(train_qs)).lower().split()\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}\n",
    "\n",
    "def tfidf_word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions combined...\n",
      "Common words found ...\n",
      "Jaccard similarities computed...\n",
      "Levenshtein distances computed...\n"
     ]
    }
   ],
   "source": [
    "def str_stem(str1):\n",
    "    str1 = str(str1)\n",
    "    str1 = re.sub(r'[^a-zA-Z0-9 ]',r'',str1)\n",
    "    str1 = str1.lower()\n",
    "    #str1 = (\" \").join([stemmer.stem(z) for z in str1.split(\" \")])\n",
    "    return str1\n",
    "\n",
    "def str_common_word(str1, str2):\n",
    "    str1, str2 = str1.lower(), str2.lower()\n",
    "    words, cnt = str1.split(), 0\n",
    "    for word in words:\n",
    "        if str2.find(word)>=0:\n",
    "            cnt+=1\n",
    "    return cnt\n",
    "def ngram(tokens, n):\n",
    "    grams =[tokens[i:i+n] for i in range(len(tokens)-(n-1))]\n",
    "    return grams\n",
    "\n",
    "def get_sim(a_tri,b_tri):\n",
    "    intersect = len(set(a_tri) & set(b_tri))\n",
    "    union = len(set(a_tri) | set(b_tri))\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    return float(intersect)/(union)\n",
    "\n",
    "def jaccard_similarity(str1,str2):\n",
    "    sentence_gram1 = str1\n",
    "    sentence_gram2 = str2\n",
    "    grams1 = ngram(sentence_gram1, 5)\n",
    "    grams2 = ngram(sentence_gram2, 5)\n",
    "    similarity = get_sim(grams1, grams2)\n",
    "    return similarity\n",
    "    \n",
    "\n",
    "df_all['question1'] = df_all['question1'].map(lambda x:str_stem(x))\n",
    "df_all['question2'] = df_all['question2'].map(lambda x:str_stem(x))\n",
    "\n",
    "df_all['len_of_q1'] = df_all['question1'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "df_all['len_of_q2'] = df_all['question2'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "\n",
    "df_all['questions'] = df_all['question1']+\"|\"+df_all['question2']\n",
    "print (\"Questions combined...\")\n",
    "df_all['q2_in_q1'] = df_all['questions'].map(lambda x:str_common_word(x.split('|')[0],x.split('|')[1]))\n",
    "df_all['q1_in_q2'] = df_all['questions'].map(lambda x:str_common_word(x.split('|')[1],x.split('|')[0]))\n",
    "print (\"Common words found ...\")\n",
    "df_all['jaccard'] = df_all['questions'].map(lambda x:jaccard_similarity(x.split('|')[0],x.split('|')[1]))\n",
    "print (\"Jaccard similarities computed...\")\n",
    "df_all['lev_distance'] = df_all['questions'].map(lambda x:normalized_damerau_levenshtein_distance(x.split('|')[0],x.split('|')[1]))\n",
    "print (\"Levenshtein distances computed...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>test_id</th>\n",
       "      <th>unigrams_ques1</th>\n",
       "      <th>unigrams_ques2</th>\n",
       "      <th>unigrams_common_count</th>\n",
       "      <th>...</th>\n",
       "      <th>bigrams_common_ratio</th>\n",
       "      <th>words_ques1</th>\n",
       "      <th>words_ques2</th>\n",
       "      <th>len_of_q1</th>\n",
       "      <th>len_of_q2</th>\n",
       "      <th>questions</th>\n",
       "      <th>q2_in_q1</th>\n",
       "      <th>q1_in_q2</th>\n",
       "      <th>jaccard</th>\n",
       "      <th>lev_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[step, step, guide, invest, share, market, ind...</td>\n",
       "      <td>[step, step, guide, invest, share, market, ?]</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.138462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>what is the story of kohinoor kohinoor diamond</td>\n",
       "      <td>what would happen if the indian government sto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[story, kohinoor, (, koh-i-noor, ), diamond, ?]</td>\n",
       "      <td>[would, happen, indian, government, stole, koh...</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>what is the story of kohinoor kohinoor diamond...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.506024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>how can i increase the speed of my internet co...</td>\n",
       "      <td>how can internet speed be increased by hacking...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[increase, speed, internet, connection, using,...</td>\n",
       "      <td>[internet, speed, increased, hacking, dns, ?]</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>how can i increase the speed of my internet co...</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.184466</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>why am i mentally very lonely how can i solve it</td>\n",
       "      <td>find the remainder when math2324math is divide...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[mentally, lonely, ?, solve, ?]</td>\n",
       "      <td>[find, remainder, [, math, ], 23^, {, 24, }, [...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>why am i mentally very lonely how can i solve ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>which one dissolve in water quikly sugar salt ...</td>\n",
       "      <td>which fish would survive in salt water</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[one, dissolve, water, quikly, sugar, ,, salt,...</td>\n",
       "      <td>[fish, would, survive, salt, water, ?]</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>which one dissolve in water quikly sugar salt ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.084211</td>\n",
       "      <td>0.698630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  is_duplicate  qid1  qid2  \\\n",
       "0  0.0           0.0   1.0   2.0   \n",
       "1  1.0           0.0   3.0   4.0   \n",
       "2  2.0           0.0   5.0   6.0   \n",
       "3  3.0           0.0   7.0   8.0   \n",
       "4  4.0           0.0   9.0  10.0   \n",
       "\n",
       "                                           question1  \\\n",
       "0  what is the step by step guide to invest in sh...   \n",
       "1     what is the story of kohinoor kohinoor diamond   \n",
       "2  how can i increase the speed of my internet co...   \n",
       "3   why am i mentally very lonely how can i solve it   \n",
       "4  which one dissolve in water quikly sugar salt ...   \n",
       "\n",
       "                                           question2  test_id  \\\n",
       "0  what is the step by step guide to invest in sh...      NaN   \n",
       "1  what would happen if the indian government sto...      NaN   \n",
       "2  how can internet speed be increased by hacking...      NaN   \n",
       "3  find the remainder when math2324math is divide...      NaN   \n",
       "4             which fish would survive in salt water      NaN   \n",
       "\n",
       "                                      unigrams_ques1  \\\n",
       "0  [step, step, guide, invest, share, market, ind...   \n",
       "1    [story, kohinoor, (, koh-i-noor, ), diamond, ?]   \n",
       "2  [increase, speed, internet, connection, using,...   \n",
       "3                    [mentally, lonely, ?, solve, ?]   \n",
       "4  [one, dissolve, water, quikly, sugar, ,, salt,...   \n",
       "\n",
       "                                      unigrams_ques2  unigrams_common_count  \\\n",
       "0      [step, step, guide, invest, share, market, ?]                      6   \n",
       "1  [would, happen, indian, government, stole, koh...                      6   \n",
       "2      [internet, speed, increased, hacking, dns, ?]                      3   \n",
       "3  [find, remainder, [, math, ], 23^, {, 24, }, [...                      1   \n",
       "4             [fish, would, survive, salt, water, ?]                      3   \n",
       "\n",
       "      ...       bigrams_common_ratio words_ques1 words_ques2  len_of_q1  \\\n",
       "0     ...                   0.625000           8           7         14   \n",
       "1     ...                   0.307692           7          12          8   \n",
       "2     ...                   0.000000           7           6         14   \n",
       "3     ...                   0.000000           5          15         11   \n",
       "4     ...                   0.000000          13           6         13   \n",
       "\n",
       "   len_of_q2                                          questions  q2_in_q1  \\\n",
       "0         12  what is the step by step guide to invest in sh...        13   \n",
       "1         13  what is the story of kohinoor kohinoor diamond...         5   \n",
       "2         10  how can i increase the speed of my internet co...         7   \n",
       "3          9  why am i mentally very lonely how can i solve ...         2   \n",
       "4          7  which one dissolve in water quikly sugar salt ...         4   \n",
       "\n",
       "   q1_in_q2   jaccard lev_distance  \n",
       "0        12  0.862069     0.138462  \n",
       "1         6  0.200000     0.506024  \n",
       "2         4  0.184466     0.555556  \n",
       "3         0  0.000000     0.800000  \n",
       "4         4  0.084211     0.698630  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.to_csv(\"../data/train_test.csv\")\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srsasank/.local/lib/python3.5/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/srsasank/.local/lib/python3.5/site-packages/ipykernel/__main__.py:37: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/srsasank/.local/lib/python3.5/site-packages/ipykernel/__main__.py:37: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/home/srsasank/.local/lib/python3.5/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/srsasank/.local/lib/python3.5/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/srsasank/.local/lib/python3.5/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#df_all = df_all.drop(['id','qid1','qid2','questions','unigrams_ques1','unigrams_ques2','bigrams_ques1','bigrams_ques2'],axis=1)\n",
    "df_train = df_all.iloc[:num_train]\n",
    "df_test = df_all.iloc[num_train:]\n",
    "\n",
    "df_train['word_match'] = df_train.apply(word_match_share, axis=1, raw=True)\n",
    "df_train['tfidf_word_match'] = df_train.apply(tfidf_word_match_share, axis=1, raw=True)\n",
    "df_test['word_match'] = df_test.apply(word_match_share, axis=1, raw=True)\n",
    "df_test['tfidf_word_match'] = df_test.apply(tfidf_word_match_share, axis=1, raw=True)\n",
    "id_test = df_test['test_id']\n",
    "\n",
    "\n",
    "y_train = df_train['is_duplicate'].values\n",
    "X_train = df_train.drop(['test_id','is_duplicate','question1','question2'],axis=1).values\n",
    "X_test = df_test.drop(['test_id','is_duplicate','question1','question2'],axis=1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>test_id</th>\n",
       "      <th>unigrams_common_count</th>\n",
       "      <th>unigrams_common_ratio</th>\n",
       "      <th>bigrams_common_count</th>\n",
       "      <th>bigrams_common_ratio</th>\n",
       "      <th>words_ques1</th>\n",
       "      <th>words_ques2</th>\n",
       "      <th>len_of_q1</th>\n",
       "      <th>len_of_q2</th>\n",
       "      <th>q2_in_q1</th>\n",
       "      <th>q1_in_q2</th>\n",
       "      <th>jaccard</th>\n",
       "      <th>lev_distance</th>\n",
       "      <th>word_match</th>\n",
       "      <th>tfidf_word_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>5</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.138462</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.950123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>what is the story of kohinoor kohinoor diamond</td>\n",
       "      <td>what would happen if the indian government sto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>4</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.506024</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.448403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>how can i increase the speed of my internet co...</td>\n",
       "      <td>how can internet speed be increased by hacking...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.184466</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.355658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>why am i mentally very lonely how can i solve it</td>\n",
       "      <td>find the remainder when math2324math is divide...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>which one dissolve in water quikly sugar salt ...</td>\n",
       "      <td>which fish would survive in salt water</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.084211</td>\n",
       "      <td>0.698630</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.269607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate                                          question1  \\\n",
       "0           0.0  what is the step by step guide to invest in sh...   \n",
       "1           0.0     what is the story of kohinoor kohinoor diamond   \n",
       "2           0.0  how can i increase the speed of my internet co...   \n",
       "3           0.0   why am i mentally very lonely how can i solve it   \n",
       "4           0.0  which one dissolve in water quikly sugar salt ...   \n",
       "\n",
       "                                           question2  test_id  \\\n",
       "0  what is the step by step guide to invest in sh...      NaN   \n",
       "1  what would happen if the indian government sto...      NaN   \n",
       "2  how can internet speed be increased by hacking...      NaN   \n",
       "3  find the remainder when math2324math is divide...      NaN   \n",
       "4             which fish would survive in salt water      NaN   \n",
       "\n",
       "   unigrams_common_count  unigrams_common_ratio  bigrams_common_count  \\\n",
       "0                      6               0.857143                     5   \n",
       "1                      6               0.461538                     4   \n",
       "2                      3               0.300000                     0   \n",
       "3                      1               0.062500                     0   \n",
       "4                      3               0.200000                     0   \n",
       "\n",
       "   bigrams_common_ratio  words_ques1  words_ques2  len_of_q1  len_of_q2  \\\n",
       "0              0.625000            8            7         14         12   \n",
       "1              0.307692            7           12          8         13   \n",
       "2              0.000000            7            6         14         10   \n",
       "3              0.000000            5           15         11          9   \n",
       "4              0.000000           13            6         13          7   \n",
       "\n",
       "   q2_in_q1  q1_in_q2   jaccard  lev_distance  word_match  tfidf_word_match  \n",
       "0        13        12  0.862069      0.138462    0.909091          0.950123  \n",
       "1         5         6  0.200000      0.506024    0.363636          0.448403  \n",
       "2         7         4  0.184466      0.555556    0.363636          0.355658  \n",
       "3         2         0  0.000000      0.800000    0.000000          0.000000  \n",
       "4         4         4  0.084211      0.698630    0.266667          0.269607  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.662423\tvalid-logloss:0.662333\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 50 rounds.\n",
      "[10]\ttrain-logloss:0.524766\tvalid-logloss:0.524528\n",
      "[20]\ttrain-logloss:0.486221\tvalid-logloss:0.486441\n",
      "[30]\ttrain-logloss:0.472478\tvalid-logloss:0.473256\n",
      "[40]\ttrain-logloss:0.465876\tvalid-logloss:0.467035\n",
      "[50]\ttrain-logloss:0.462341\tvalid-logloss:0.464068\n",
      "[60]\ttrain-logloss:0.459554\tvalid-logloss:0.461633\n",
      "[70]\ttrain-logloss:0.456873\tvalid-logloss:0.459554\n",
      "[80]\ttrain-logloss:0.454488\tvalid-logloss:0.457767\n",
      "[90]\ttrain-logloss:0.452343\tvalid-logloss:0.456232\n",
      "[100]\ttrain-logloss:0.45051\tvalid-logloss:0.45503\n",
      "[110]\ttrain-logloss:0.448934\tvalid-logloss:0.454021\n",
      "[120]\ttrain-logloss:0.447204\tvalid-logloss:0.452908\n",
      "[130]\ttrain-logloss:0.445677\tvalid-logloss:0.451965\n",
      "[140]\ttrain-logloss:0.444519\tvalid-logloss:0.451317\n",
      "[150]\ttrain-logloss:0.443428\tvalid-logloss:0.450746\n",
      "[160]\ttrain-logloss:0.442263\tvalid-logloss:0.450243\n",
      "[170]\ttrain-logloss:0.441338\tvalid-logloss:0.449834\n",
      "[180]\ttrain-logloss:0.440488\tvalid-logloss:0.449459\n",
      "[190]\ttrain-logloss:0.439456\tvalid-logloss:0.448975\n",
      "[200]\ttrain-logloss:0.438744\tvalid-logloss:0.448795\n",
      "[210]\ttrain-logloss:0.437711\tvalid-logloss:0.448313\n",
      "[220]\ttrain-logloss:0.436817\tvalid-logloss:0.447968\n",
      "[230]\ttrain-logloss:0.435936\tvalid-logloss:0.4476\n",
      "[240]\ttrain-logloss:0.435237\tvalid-logloss:0.447426\n",
      "[250]\ttrain-logloss:0.434352\tvalid-logloss:0.44713\n",
      "[260]\ttrain-logloss:0.433683\tvalid-logloss:0.446928\n",
      "[270]\ttrain-logloss:0.432938\tvalid-logloss:0.446746\n",
      "[280]\ttrain-logloss:0.432064\tvalid-logloss:0.446569\n",
      "[290]\ttrain-logloss:0.431668\tvalid-logloss:0.446447\n",
      "[300]\ttrain-logloss:0.431131\tvalid-logloss:0.44636\n",
      "[310]\ttrain-logloss:0.43035\tvalid-logloss:0.446083\n",
      "[320]\ttrain-logloss:0.429612\tvalid-logloss:0.445905\n",
      "[330]\ttrain-logloss:0.428756\tvalid-logloss:0.445607\n",
      "[340]\ttrain-logloss:0.428342\tvalid-logloss:0.445498\n",
      "[350]\ttrain-logloss:0.427671\tvalid-logloss:0.445335\n",
      "[360]\ttrain-logloss:0.427045\tvalid-logloss:0.445242\n",
      "[370]\ttrain-logloss:0.426574\tvalid-logloss:0.445177\n",
      "[380]\ttrain-logloss:0.425879\tvalid-logloss:0.444991\n",
      "[390]\ttrain-logloss:0.425346\tvalid-logloss:0.444887\n",
      "[400]\ttrain-logloss:0.424697\tvalid-logloss:0.444733\n",
      "[410]\ttrain-logloss:0.423976\tvalid-logloss:0.444568\n",
      "[420]\ttrain-logloss:0.423439\tvalid-logloss:0.44448\n",
      "[430]\ttrain-logloss:0.422949\tvalid-logloss:0.44436\n",
      "[440]\ttrain-logloss:0.422419\tvalid-logloss:0.444205\n",
      "[450]\ttrain-logloss:0.421879\tvalid-logloss:0.4441\n",
      "[460]\ttrain-logloss:0.421398\tvalid-logloss:0.444018\n",
      "[470]\ttrain-logloss:0.420958\tvalid-logloss:0.443951\n",
      "[480]\ttrain-logloss:0.420362\tvalid-logloss:0.443877\n",
      "[490]\ttrain-logloss:0.419821\tvalid-logloss:0.443714\n",
      "[500]\ttrain-logloss:0.41935\tvalid-logloss:0.443639\n",
      "[510]\ttrain-logloss:0.418783\tvalid-logloss:0.443545\n",
      "[520]\ttrain-logloss:0.418385\tvalid-logloss:0.443498\n",
      "[530]\ttrain-logloss:0.417844\tvalid-logloss:0.443381\n",
      "[540]\ttrain-logloss:0.417237\tvalid-logloss:0.443227\n",
      "[550]\ttrain-logloss:0.416747\tvalid-logloss:0.443137\n",
      "[560]\ttrain-logloss:0.416359\tvalid-logloss:0.443042\n",
      "[570]\ttrain-logloss:0.415753\tvalid-logloss:0.442895\n",
      "[580]\ttrain-logloss:0.415243\tvalid-logloss:0.442802\n",
      "[590]\ttrain-logloss:0.414923\tvalid-logloss:0.442755\n",
      "[600]\ttrain-logloss:0.414441\tvalid-logloss:0.442731\n",
      "[610]\ttrain-logloss:0.413992\tvalid-logloss:0.442657\n",
      "[620]\ttrain-logloss:0.413569\tvalid-logloss:0.442596\n",
      "[630]\ttrain-logloss:0.412948\tvalid-logloss:0.442444\n",
      "[640]\ttrain-logloss:0.412484\tvalid-logloss:0.442361\n",
      "[650]\ttrain-logloss:0.412081\tvalid-logloss:0.442347\n",
      "[660]\ttrain-logloss:0.411655\tvalid-logloss:0.44229\n",
      "[670]\ttrain-logloss:0.411169\tvalid-logloss:0.442185\n",
      "[680]\ttrain-logloss:0.41064\tvalid-logloss:0.442132\n",
      "[690]\ttrain-logloss:0.41013\tvalid-logloss:0.442051\n",
      "[700]\ttrain-logloss:0.409566\tvalid-logloss:0.441916\n",
      "[710]\ttrain-logloss:0.40914\tvalid-logloss:0.441882\n",
      "[720]\ttrain-logloss:0.408763\tvalid-logloss:0.441851\n",
      "[730]\ttrain-logloss:0.408233\tvalid-logloss:0.441795\n",
      "[740]\ttrain-logloss:0.407863\tvalid-logloss:0.441767\n",
      "[750]\ttrain-logloss:0.407387\tvalid-logloss:0.441722\n",
      "[760]\ttrain-logloss:0.406792\tvalid-logloss:0.441619\n",
      "[770]\ttrain-logloss:0.406368\tvalid-logloss:0.441598\n",
      "[780]\ttrain-logloss:0.406008\tvalid-logloss:0.441581\n",
      "[790]\ttrain-logloss:0.405549\tvalid-logloss:0.441546\n",
      "[800]\ttrain-logloss:0.404993\tvalid-logloss:0.441531\n",
      "[810]\ttrain-logloss:0.404665\tvalid-logloss:0.441523\n",
      "[820]\ttrain-logloss:0.404313\tvalid-logloss:0.441496\n",
      "[830]\ttrain-logloss:0.403868\tvalid-logloss:0.441452\n",
      "[840]\ttrain-logloss:0.403552\tvalid-logloss:0.441479\n",
      "[850]\ttrain-logloss:0.40334\tvalid-logloss:0.441464\n",
      "[860]\ttrain-logloss:0.402816\tvalid-logloss:0.441416\n",
      "[870]\ttrain-logloss:0.402287\tvalid-logloss:0.441363\n",
      "[880]\ttrain-logloss:0.401791\tvalid-logloss:0.441282\n",
      "[890]\ttrain-logloss:0.401304\tvalid-logloss:0.441233\n",
      "[900]\ttrain-logloss:0.401032\tvalid-logloss:0.441224\n",
      "[910]\ttrain-logloss:0.400537\tvalid-logloss:0.441161\n",
      "[920]\ttrain-logloss:0.400122\tvalid-logloss:0.441062\n",
      "[930]\ttrain-logloss:0.399595\tvalid-logloss:0.441026\n",
      "[940]\ttrain-logloss:0.399303\tvalid-logloss:0.44101\n",
      "[950]\ttrain-logloss:0.39889\tvalid-logloss:0.440985\n",
      "[960]\ttrain-logloss:0.398547\tvalid-logloss:0.440982\n",
      "[970]\ttrain-logloss:0.398341\tvalid-logloss:0.440971\n",
      "[980]\ttrain-logloss:0.398016\tvalid-logloss:0.44095\n",
      "[990]\ttrain-logloss:0.397574\tvalid-logloss:0.440893\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "x_trainb, x_validb, y_trainb, y_validb = train_test_split(X_train, y_train, test_size=0.2, random_state=4747)\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.1\n",
    "params['max_depth'] = 6\n",
    "\n",
    "d_train = xgb.DMatrix(x_trainb, label=y_trainb)\n",
    "d_valid = xgb.DMatrix(x_validb, label=y_validb)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 1000, watchlist, early_stopping_rounds=50, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_test = xgb.DMatrix(X_test)\n",
    "p_test = bst.predict(d_test)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = np.int32(id_test)\n",
    "sub['is_duplicate'] = p_test\n",
    "sub.to_csv('../submission/simple_xgb_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my first notebook in Kaggle. Looking for suggestions to improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19124366100096607\n"
     ]
    }
   ],
   "source": [
    "X_train2 = df_train.drop(['test_id','is_duplicate','question1','question2'],axis=1)\n",
    "pos_train = X_train2[y_train == 1]\n",
    "neg_train = X_train2[y_train == 0]\n",
    "\n",
    "# Now we oversample the negative class\n",
    "# There is likely a much more elegant way to do this...\n",
    "p = 0.165\n",
    "scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "while scale > 1:\n",
    "    neg_train = pd.concat([neg_train, neg_train])\n",
    "    scale -=1\n",
    "neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "print(len(pos_train) / (len(pos_train) + len(neg_train)))\n",
    "\n",
    "x_train = pd.concat([pos_train, neg_train])\n",
    "y_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "del pos_train, neg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finally, we split some of the data off for validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=4242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.605861\tvalid-logloss:0.606339\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 50 rounds.\n",
      "[10]\ttrain-logloss:0.395065\tvalid-logloss:0.39737\n",
      "[20]\ttrain-logloss:0.374835\tvalid-logloss:0.37761\n",
      "[30]\ttrain-logloss:0.369183\tvalid-logloss:0.372378\n",
      "[40]\ttrain-logloss:0.364991\tvalid-logloss:0.368408\n",
      "[50]\ttrain-logloss:0.362436\tvalid-logloss:0.366212\n",
      "[60]\ttrain-logloss:0.360412\tvalid-logloss:0.364427\n",
      "[70]\ttrain-logloss:0.358777\tvalid-logloss:0.362943\n",
      "[80]\ttrain-logloss:0.357776\tvalid-logloss:0.362138\n",
      "[90]\ttrain-logloss:0.356655\tvalid-logloss:0.361191\n",
      "[100]\ttrain-logloss:0.355655\tvalid-logloss:0.360372\n",
      "[110]\ttrain-logloss:0.354762\tvalid-logloss:0.359672\n",
      "[120]\ttrain-logloss:0.35407\tvalid-logloss:0.359099\n",
      "[130]\ttrain-logloss:0.353003\tvalid-logloss:0.358293\n",
      "[140]\ttrain-logloss:0.352487\tvalid-logloss:0.357926\n",
      "[150]\ttrain-logloss:0.351797\tvalid-logloss:0.357441\n",
      "[160]\ttrain-logloss:0.35125\tvalid-logloss:0.357034\n",
      "[170]\ttrain-logloss:0.350681\tvalid-logloss:0.356616\n",
      "[180]\ttrain-logloss:0.350328\tvalid-logloss:0.356402\n",
      "[190]\ttrain-logloss:0.349765\tvalid-logloss:0.355977\n",
      "[200]\ttrain-logloss:0.349243\tvalid-logloss:0.355624\n",
      "[210]\ttrain-logloss:0.348964\tvalid-logloss:0.355445\n",
      "[220]\ttrain-logloss:0.348964\tvalid-logloss:0.355445\n",
      "[230]\ttrain-logloss:0.348964\tvalid-logloss:0.355445\n",
      "[240]\ttrain-logloss:0.348964\tvalid-logloss:0.355445\n",
      "[250]\ttrain-logloss:0.348964\tvalid-logloss:0.355445\n",
      "Stopping. Best iteration:\n",
      "[208]\ttrain-logloss:0.348964\tvalid-logloss:0.355445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Set our parameters for xgboost\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.2\n",
    "params['max_depth'] = 4\n",
    "params['gamma'] =5\n",
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(x_valid, label=y_valid)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 5000, watchlist, early_stopping_rounds=50, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = df_test.drop(['test_id','is_duplicate','question1','question2'],axis=1)\n",
    "d_test = xgb.DMatrix(X_test)\n",
    "p_test = bst.predict(d_test)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = np.int32(id_test)\n",
    "sub['is_duplicate'] = p_test\n",
    "sub.to_csv('../submission/simple_xgb_v5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
